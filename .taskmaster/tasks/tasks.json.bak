{
  "tasks": [
    {
      "id": 1,
      "title": "Setup Project Structure and Environment",
      "description": "Initialize the project repository with the required directory structure for both backend and frontend, and configure the development environment with necessary dependencies.",
      "details": "1. Create the project directory structure as specified in section 9.2:\n   - poc_gemini_ha/\n     - backend/\n       - poc_app/\n         - main.py\n         - gemini_client/\n         - ha_client/\n         - core/\n         - models/\n       - tests/\n       - .env\n       - requirements.txt\n     - frontend/\n       - src/\n         - components/\n         - composables/\n         - store/\n         - views/\n       - tests/\n       - package.json\n     - README.md\n\n2. Backend setup:\n   - Initialize Python virtual environment\n   - Install required packages: FastAPI, uvicorn, httpx, google-genai, websockets, python-dotenv, pydantic\n   - Create .env file with placeholders for environment variables specified in section 9.1\n   - Setup basic FastAPI application in main.py\n\n3. Frontend setup:\n   - Initialize Vue3 project with Vue CLI or Vite\n   - Configure TypeScript\n   - Install dependencies: Pinia, Vue Router, vue-audio-visual\n   - Setup basic project structure\n\n4. Create README.md with project overview and setup instructions",
      "testStrategy": "1. Verify all directories and files are created according to the structure\n2. Ensure backend server starts without errors\n3. Ensure frontend development server starts without errors\n4. Validate that all dependencies are correctly installed and importable\n5. Confirm environment variables are properly loaded from .env file",
      "priority": "high",
      "dependencies": [],
      "status": "done",
      "subtasks": [
        {
          "id": 1,
          "title": "Create Project Directory Structure",
          "description": "Set up the initial directory structure for both backend and frontend components of the project.",
          "dependencies": [],
          "details": "Create the main project directory 'poc_gemini_ha' with subdirectories for backend (including poc_app, tests) and frontend (including src, tests). Ensure all specified subdirectories and files are created as per the structure in section 9.2.\n<info added on 2025-06-08T00:33:03.884Z>\nA estrutura de diret√≥rios foi implementada com sucesso:\n\n- backend/\n  - poc_app/ (com __init__.py)\n    - gemini_client/ (com __init__.py)\n    - ha_client/ (com __init__.py)\n    - core/ (com __init__.py)\n    - models/ (com __init__.py)\n  - tests/ (com __init__.py)\n- frontend/\n  - src/\n    - components/\n    - composables/\n    - store/\n    - views/\n  - tests/\n\nTodos os diret√≥rios foram criados conforme especificado no PRD (se√ß√£o 9.2). Os arquivos __init__.py foram adicionados aos pacotes Python para torn√°-los import√°veis.\n</info added on 2025-06-08T00:33:03.884Z>",
          "status": "done",
          "testStrategy": "Verify the existence and correct hierarchy of all directories and files using a shell script or Python script that checks the file system."
        },
        {
          "id": 2,
          "title": "Initialize Backend Environment",
          "description": "Set up the Python virtual environment and install required packages for the backend.",
          "dependencies": [
            1
          ],
          "details": "Create a Python virtual environment in the backend directory. Install FastAPI, uvicorn, httpx, google-genai, websockets, python-dotenv, and pydantic using pip. Create a requirements.txt file listing all installed packages and their versions.\n<info added on 2025-06-08T00:37:31.347Z>\n‚úÖ Ambiente backend inicializado com sucesso!\n\n**Realiza√ß√µes:**\n1. **Ambiente Virtual Python 3.11:** Criado em `backend/venv/`\n2. **Depend√™ncias Instaladas:** Todas as depend√™ncias do PRD foram instaladas com sucesso:\n   - fastapi==0.115.12\n   - uvicorn==0.34.3 \n   - httpx==0.28.1\n   - google-genai==1.19.0 ‚úÖ (SDK correto para Gemini Live API)\n   - websockets==15.0.1\n   - python-dotenv==1.1.0\n   - pydantic==2.11.5\n   - E todas as depend√™ncias necess√°rias\n\n3. **Requirements.txt:** Gerado com todas as vers√µes das depend√™ncias\n4. **Teste de Importa√ß√£o:** Todas as depend√™ncias principais foram importadas com sucesso\n\n**Pr√≥ximo passo:** Configurar aplica√ß√£o backend b√°sica (subtarefa 1.3)\n</info added on 2025-06-08T00:37:31.347Z>",
          "status": "done",
          "testStrategy": "Run 'pip freeze' to verify all required packages are installed. Attempt to import each package in a Python script to ensure they're accessible."
        },
        {
          "id": 3,
          "title": "Configure Backend Application",
          "description": "Set up the basic FastAPI application and environment variables.",
          "dependencies": [
            2
          ],
          "details": "Create a .env file in the backend directory with placeholders for environment variables as specified in section 9.1. Implement a basic FastAPI application in main.py that loads these environment variables using python-dotenv.\n<info added on 2025-06-08T00:43:47.472Z>\n## Backend Application Successfully Configured\n\n### Accomplishments:\n1. **Created .env file:** With all necessary environment variables as per PRD section 9.1:\n   - GEMINI_API_KEY (placeholder)\n   - HA_URL and HA_LLAT (placeholders)\n   - AUDIO_SAMPLE_RATE_GEMINI=16000\n   - AUDIO_CHANNELS_GEMINI=1\n\n2. **Configuration module (core/config.py):** \n   - Implemented with Pydantic Settings\n   - Automatic loading of environment variables\n   - Absolute path to .env file working correctly\n\n3. **FastAPI Application (main.py):**\n   - Basic application created with title and description\n   - CORS configured for Vue3 frontend\n   - Implemented endpoints: `/` (root), `/health`, `/ws/voice` (WebSocket)\n   - Logging configured\n   - Server starts correctly on port 8000\n\n4. **Tests performed:**\n   - Configurations loaded successfully\n   - FastAPI application created without errors\n   - Server starts and responds correctly\n</info added on 2025-06-08T00:43:47.472Z>",
          "status": "done",
          "testStrategy": "Run the FastAPI application and make a test request to the root endpoint to ensure it's working. Verify that environment variables are correctly loaded."
        },
        {
          "id": 4,
          "title": "Initialize Frontend Project",
          "description": "Set up the Vue3 project with necessary configurations and dependencies.",
          "dependencies": [
            1
          ],
          "details": "Use Vue CLI or Vite to initialize a new Vue3 project in the frontend directory. Configure TypeScript support. Install Pinia, Vue Router, and vue-audio-visual using npm or yarn. Set up the basic project structure including components, composables, store, and views directories.\n<info added on 2025-06-08T00:48:01.821Z>\n‚úÖ Projeto frontend inicializado com sucesso!\n\n**Realiza√ß√µes:**\n1. **Projeto Vue3 criado:** Usando Vite como bundler (mais r√°pido que Vue CLI)\n2. **TypeScript configurado:** \n   - tsconfig.json e tsconfig.node.json criados\n   - main.js convertido para main.ts\n   - Declara√ß√µes de tipos para arquivos .vue\n   - Compila√ß√£o TypeScript funcionando\n\n3. **Depend√™ncias instaladas:**\n   - Vue 3 (framework principal)\n   - TypeScript (tipagem est√°tica)\n   - Pinia (gerenciamento de estado)\n   - Vue Router 4 (roteamento)\n   - vue-audio-visual (visualiza√ß√£o de √°udio)\n\n4. **Estrutura de diret√≥rios criada:**\n   - src/components/ ‚úÖ\n   - src/composables/ ‚úÖ\n   - src/store/ ‚úÖ\n   - src/views/ ‚úÖ\n   - src/tests/ ‚úÖ\n\n5. **Configura√ß√£o b√°sica implementada:**\n   - main.ts configurado com Pinia e Vue Router\n   - App.vue atualizado para usar router-view\n   - HomeView.vue criada com interface inicial\n   - Estilos b√°sicos aplicados\n\n6. **Testes realizados:**\n   - Projeto compila sem erros (npm run build)\n   - Build de produ√ß√£o gerado com sucesso\n   - Estrutura pronta para desenvolvimento\n\n**Pr√≥ximo passo:** Criar documenta√ß√£o do projeto (subtarefa 1.5)\n</info added on 2025-06-08T00:48:01.821Z>",
          "status": "done",
          "testStrategy": "Run the Vue development server and verify that the application compiles without errors. Check that all installed packages are listed in package.json."
        },
        {
          "id": 5,
          "title": "Create Project Documentation",
          "description": "Write the initial README.md file with project overview and setup instructions.",
          "dependencies": [
            1,
            2,
            3,
            4
          ],
          "details": "Create a README.md file in the project root directory. Include sections for project overview, technologies used, directory structure, setup instructions for both backend and frontend, and any other relevant information for developers.\n<info added on 2025-06-08T00:49:14.333Z>\nA documenta√ß√£o do projeto foi criada com sucesso, incluindo um README.md completo no diret√≥rio raiz. O documento cont√©m todas as se√ß√µes planejadas: vis√£o geral do projeto, tecnologias utilizadas, estrutura de diret√≥rios, instru√ß√µes de configura√ß√£o para backend e frontend, al√©m de informa√ß√µes adicionais como endpoints da API, fluxo de funcionamento, comandos de desenvolvimento, status do projeto, guia de contribui√ß√£o e suporte. A documenta√ß√£o t√©cnica foi elaborada com detalhes sobre estrutura de arquivos, depend√™ncias, configura√ß√£o do ambiente e obten√ß√£o de chaves de API. O formato foi aprimorado com emojis, blocos de c√≥digo com syntax highlighting e links para recursos externos, resultando em uma documenta√ß√£o profissional e abrangente que permite a qualquer desenvolvedor configurar e executar o projeto facilmente.\n</info added on 2025-06-08T00:49:14.333Z>",
          "status": "done",
          "testStrategy": "Review the README.md file to ensure all required sections are present and information is accurate. Have another team member attempt to set up the project using only the README instructions."
        }
      ]
    },
    {
      "id": 2,
      "title": "Implement Backend WebSocket Server",
      "description": "Develop the WebSocket server in the backend to handle real-time communication between the frontend and the backend services.",
      "details": "1. Create WebSocket endpoint in FastAPI:\n```python\n@app.websocket(\"/ws\")\nasync def websocket_endpoint(websocket: WebSocket):\n    await websocket.accept()\n    try:\n        while True:\n            # Receive audio data from frontend\n            data = await websocket.receive_bytes()\n            # Process data (will be implemented in subsequent tasks)\n            # Send response back to frontend\n            await websocket.send_bytes(response_data)\n    except WebSocketDisconnect:\n        logger.info(\"WebSocket disconnected\")\n    except Exception as e:\n        logger.error(f\"WebSocket error: {str(e)}\")\n        await websocket.close()\n```\n\n2. Implement connection manager to handle multiple concurrent WebSocket connections:\n```python\nclass ConnectionManager:\n    def __init__(self):\n        self.active_connections: List[WebSocket] = []\n\n    async def connect(self, websocket: WebSocket):\n        await websocket.accept()\n        self.active_connections.append(websocket)\n\n    def disconnect(self, websocket: WebSocket):\n        self.active_connections.remove(websocket)\n\n    async def send_bytes(self, message: bytes, websocket: WebSocket):\n        await websocket.send_bytes(message)\n```\n\n3. Implement error handling and reconnection logic\n4. Add logging for connection events and errors\n5. Create basic message protocol for different types of messages (audio data, text, status updates)",
      "testStrategy": "1. Use WebSocket testing tools (like Postman or wscat) to verify connection establishment\n2. Test sending and receiving binary data through the WebSocket\n3. Verify proper error handling when connection is interrupted\n4. Test multiple concurrent connections\n5. Validate logging of connection events",
      "priority": "high",
      "dependencies": [
        1
      ],
      "status": "in-progress",
      "subtasks": []
    },
    {
      "id": 3,
      "title": "Implement Home Assistant Client Module",
      "description": "Develop the Home Assistant client module to communicate with the Home Assistant REST API for controlling devices and retrieving information.",
      "details": "1. Create a Home Assistant client class in ha_client/client.py:\n```python\nclass HomeAssistantClient:\n    def __init__(self, base_url: str, access_token: str):\n        self.base_url = base_url\n        self.headers = {\n            \"Authorization\": f\"Bearer {access_token}\",\n            \"Content-Type\": \"application/json\"\n        }\n        self.client = httpx.AsyncClient(headers=self.headers, timeout=10.0)\n    \n    async def get_entity_state(self, entity_id: str) -> dict:\n        \"\"\"Get the current state of an entity\"\"\"\n        response = await self.client.get(f\"{self.base_url}/api/states/{entity_id}\")\n        response.raise_for_status()\n        return response.json()\n    \n    async def call_service(self, domain: str, service: str, service_data: dict) -> dict:\n        \"\"\"Call a Home Assistant service\"\"\"\n        response = await self.client.post(\n            f\"{self.base_url}/api/services/{domain}/{service}\",\n            json=service_data\n        )\n        response.raise_for_status()\n        return response.json()\n```\n\n2. Implement specific service methods for required functionality:\n```python\nasync def control_light(self, entity_id: str, state: str, brightness: Optional[int] = None, color: Optional[List[int]] = None) -> dict:\n    \"\"\"Control a light entity\"\"\"\n    service_data = {\"entity_id\": entity_id}\n    \n    if brightness is not None:\n        service_data[\"brightness\"] = brightness\n    \n    if color is not None:\n        service_data[\"rgb_color\"] = color\n    \n    return await self.call_service(\"light\", \"turn_\" + state, service_data)\n\nasync def control_switch(self, entity_id: str, state: str) -> dict:\n    \"\"\"Control a switch entity\"\"\"\n    service_data = {\"entity_id\": entity_id}\n    return await self.call_service(\"switch\", \"turn_\" + state, service_data)\n\nasync def activate_scene(self, scene_id: str) -> dict:\n    \"\"\"Activate a scene\"\"\"\n    service_data = {\"entity_id\": scene_id}\n    return await self.call_service(\"scene\", \"turn_on\", service_data)\n```\n\n3. Implement error handling and retry logic\n4. Add validation for input parameters using Pydantic models\n5. Create factory method to instantiate client from environment variables",
      "testStrategy": "1. Create mock Home Assistant API responses for unit testing\n2. Test each service method with valid parameters\n3. Test error handling with invalid parameters and API errors\n4. Verify retry logic works correctly\n5. Test with actual Home Assistant instance in development environment\n6. Validate that all required functions from section 4.2 are implemented and working",
      "priority": "high",
      "dependencies": [
        1
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 4,
      "title": "Implement Gemini Live API Client",
      "description": "Develop the Gemini client module to handle communication with Google's Gemini Live API for processing voice commands and generating responses.",
      "details": "1. Create a Gemini client class in gemini_client/client.py:\n```python\nclass GeminiLiveClient:\n    def __init__(self, api_key: str):\n        self.api_key = api_key\n        self.genai = genai.GenerativeModel(\n            model_name=\"gemini-2.5-flash-preview-native-audio-dialog\",\n            generation_config={\n                \"temperature\": 0.2,\n                \"max_output_tokens\": 1024,\n            }\n        )\n        \n    async def start_audio_session(self, function_declarations):\n        \"\"\"Start a new audio streaming session with function calling\"\"\"\n        session = self.genai.start_chat(tools=function_declarations)\n        return session\n        \n    async def process_audio_chunk(self, session, audio_chunk):\n        \"\"\"Process an audio chunk and return the response\"\"\"\n        response = await session.send_audio(audio_chunk)\n        return response\n```\n\n2. Define function declarations for Home Assistant control:\n```python\nHA_FUNCTION_DECLARATIONS = [\n    {\n        \"name\": \"control_light\",\n        \"description\": \"Control a light entity in Home Assistant\",\n        \"parameters\": {\n            \"type\": \"object\",\n            \"properties\": {\n                \"entity_id\": {\n                    \"type\": \"string\",\n                    \"description\": \"The entity ID of the light to control\"\n                },\n                \"state\": {\n                    \"type\": \"string\",\n                    \"enum\": [\"on\", \"off\"],\n                    \"description\": \"The desired state of the light\"\n                },\n                \"brightness\": {\n                    \"type\": \"integer\",\n                    \"minimum\": 0,\n                    \"maximum\": 255,\n                    \"description\": \"The brightness level (0-255)\"\n                },\n                \"color\": {\n                    \"type\": \"array\",\n                    \"items\": {\n                        \"type\": \"integer\",\n                        \"minimum\": 0,\n                        \"maximum\": 255\n                    },\n                    \"description\": \"RGB color values as [R, G, B]\"\n                }\n            },\n            \"required\": [\"entity_id\", \"state\"]\n        }\n    },\n    # Add other function declarations for switch, scene, etc.\n]\n```\n\n3. Implement function to handle function calling responses:\n```python\nasync def handle_function_call(self, response, ha_client):\n    \"\"\"Handle function calls from Gemini and execute them via Home Assistant client\"\"\"\n    if not response.candidates[0].content.parts[0].function_call:\n        return None\n        \n    function_call = response.candidates[0].content.parts[0].function_call\n    function_name = function_call.name\n    function_args = function_call.args\n    \n    # Map function calls to Home Assistant client methods\n    if function_name == \"control_light\":\n        return await ha_client.control_light(**function_args)\n    elif function_name == \"control_switch\":\n        return await ha_client.control_switch(**function_args)\n    # Add other function mappings\n```\n\n4. Implement audio response handling:\n```python\nasync def get_audio_response(self, session, function_result):\n    \"\"\"Get audio response after function execution\"\"\"\n    response = await session.send_message(f\"Function executed with result: {function_result}\")\n    return response.audio\n```\n\n5. Add error handling, reconnection logic, and session management",
      "testStrategy": "1. Create mock Gemini API responses for unit testing\n2. Test audio session initialization with function declarations\n3. Test processing of audio chunks with sample audio data\n4. Verify function calling detection and parsing\n5. Test integration with Home Assistant client using mocks\n6. Validate audio response generation\n7. Test error handling and session management",
      "priority": "high",
      "dependencies": [
        1
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 5,
      "title": "Implement Frontend Audio Capture",
      "description": "Develop the frontend functionality to capture audio from the user's microphone and stream it to the backend via WebSocket.",
      "details": "1. Create an audio capture composable in frontend/src/composables/useAudioCapture.ts:\n```typescript\nimport { ref, onUnmounted } from 'vue'\n\nexport function useAudioCapture() {\n  const isRecording = ref(false)\n  const audioContext = ref<AudioContext | null>(null)\n  const mediaStream = ref<MediaStream | null>(null)\n  const processor = ref<ScriptProcessorNode | null>(null)\n  const websocket = ref<WebSocket | null>(null)\n  \n  const startRecording = async (wsUrl: string) => {\n    try {\n      // Initialize WebSocket\n      websocket.value = new WebSocket(wsUrl)\n      \n      await new Promise((resolve, reject) => {\n        websocket.value!.onopen = resolve\n        websocket.value!.onerror = reject\n      })\n      \n      // Initialize audio context and stream\n      audioContext.value = new AudioContext()\n      mediaStream.value = await navigator.mediaDevices.getUserMedia({\n        audio: {\n          channelCount: 1,\n          sampleRate: 16000\n        }\n      })\n      \n      // Create processor to handle audio data\n      processor.value = audioContext.value.createScriptProcessor(4096, 1, 1)\n      \n      // Connect audio nodes\n      const source = audioContext.value.createMediaStreamSource(mediaStream.value)\n      source.connect(processor.value)\n      processor.value.connect(audioContext.value.destination)\n      \n      // Process audio data\n      processor.value.onaudioprocess = (e) => {\n        if (!isRecording.value) return\n        \n        const inputData = e.inputBuffer.getChannelData(0)\n        const pcmData = convertFloat32ToInt16(inputData)\n        \n        if (websocket.value?.readyState === WebSocket.OPEN) {\n          websocket.value.send(pcmData)\n        }\n      }\n      \n      isRecording.value = true\n    } catch (error) {\n      console.error('Error starting recording:', error)\n      throw error\n    }\n  }\n  \n  const stopRecording = () => {\n    isRecording.value = false\n    \n    if (processor.value) {\n      processor.value.disconnect()\n      processor.value = null\n    }\n    \n    if (mediaStream.value) {\n      mediaStream.value.getTracks().forEach(track => track.stop())\n      mediaStream.value = null\n    }\n    \n    if (audioContext.value) {\n      audioContext.value.close()\n      audioContext.value = null\n    }\n    \n    if (websocket.value) {\n      websocket.value.close()\n      websocket.value = null\n    }\n  }\n  \n  // Convert Float32Array to Int16Array for PCM format\n  const convertFloat32ToInt16 = (buffer: Float32Array) => {\n    const l = buffer.length\n    const buf = new Int16Array(l)\n    \n    for (let i = 0; i < l; i++) {\n      buf[i] = Math.min(1, Math.max(-1, buffer[i])) * 0x7FFF\n    }\n    \n    return buf.buffer\n  }\n  \n  onUnmounted(() => {\n    stopRecording()\n  })\n  \n  return {\n    isRecording,\n    startRecording,\n    stopRecording\n  }\n}\n```\n\n2. Create an audio visualization component using vue-audio-visual:\n```vue\n<template>\n  <div class=\"audio-visualizer\">\n    <canvas ref=\"canvas\" :width=\"width\" :height=\"height\"></canvas>\n  </div>\n</template>\n\n<script setup lang=\"ts\">\nimport { ref, onMounted, onUnmounted, watch } from 'vue'\n\nconst props = defineProps<{\n  mediaStream: MediaStream | null\n  width: number\n  height: number\n}>()\n\nconst canvas = ref<HTMLCanvasElement | null>(null)\nlet animationFrame: number | null = null\nlet audioContext: AudioContext | null = null\nlet analyser: AnalyserNode | null = null\nlet dataArray: Uint8Array | null = null\n\nconst setupAnalyser = () => {\n  if (!props.mediaStream) return\n  \n  audioContext = new AudioContext()\n  analyser = audioContext.createAnalyser()\n  analyser.fftSize = 256\n  \n  const source = audioContext.createMediaStreamSource(props.mediaStream)\n  source.connect(analyser)\n  \n  const bufferLength = analyser.frequencyBinCount\n  dataArray = new Uint8Array(bufferLength)\n  \n  draw()\n}\n\nconst draw = () => {\n  if (!canvas.value || !analyser || !dataArray) return\n  \n  animationFrame = requestAnimationFrame(draw)\n  \n  const canvasCtx = canvas.value.getContext('2d')\n  if (!canvasCtx) return\n  \n  analyser.getByteFrequencyData(dataArray)\n  \n  canvasCtx.fillStyle = 'rgb(0, 0, 0)'\n  canvasCtx.fillRect(0, 0, props.width, props.height)\n  \n  const barWidth = (props.width / dataArray.length) * 2.5\n  let barHeight: number\n  let x = 0\n  \n  for (let i = 0; i < dataArray.length; i++) {\n    barHeight = dataArray[i] / 2\n    \n    canvasCtx.fillStyle = `rgb(${barHeight + 100}, 50, 50)`\n    canvasCtx.fillRect(x, props.height - barHeight, barWidth, barHeight)\n    \n    x += barWidth + 1\n  }\n}\n\nwatch(() => props.mediaStream, (newStream) => {\n  if (newStream) {\n    setupAnalyser()\n  }\n})\n\nonMounted(() => {\n  if (props.mediaStream) {\n    setupAnalyser()\n  }\n})\n\nonUnmounted(() => {\n  if (animationFrame) {\n    cancelAnimationFrame(animationFrame)\n  }\n  \n  if (audioContext) {\n    audioContext.close()\n  }\n})\n</script>\n```\n\n3. Implement error handling and reconnection logic for WebSocket\n4. Add visual indicators for recording state\n5. Ensure proper cleanup of audio resources when component is unmounted",
      "testStrategy": "1. Test microphone access permissions\n2. Verify audio capture starts and stops correctly\n3. Test WebSocket connection establishment and data transmission\n4. Validate audio format conversion (Float32 to Int16)\n5. Test visualization component with sample audio data\n6. Verify resource cleanup on component unmount\n7. Test across different browsers for compatibility",
      "priority": "medium",
      "dependencies": [
        1
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 6,
      "title": "Implement Frontend Audio Playback",
      "description": "Develop the frontend functionality to receive audio responses from the backend and play them back to the user.",
      "details": "1. Create an audio playback composable in frontend/src/composables/useAudioPlayback.ts:\n```typescript\nimport { ref } from 'vue'\n\nexport function useAudioPlayback() {\n  const isPlaying = ref(false)\n  const audioContext = ref<AudioContext | null>(null)\n  \n  const initAudioContext = () => {\n    if (!audioContext.value) {\n      audioContext.value = new AudioContext()\n    }\n    return audioContext.value\n  }\n  \n  const playAudioBuffer = async (audioData: ArrayBuffer) => {\n    try {\n      const context = initAudioContext()\n      isPlaying.value = true\n      \n      // Decode the audio data\n      const audioBuffer = await context.decodeAudioData(audioData)\n      \n      // Create buffer source\n      const source = context.createBufferSource()\n      source.buffer = audioBuffer\n      source.connect(context.destination)\n      \n      // Play the audio\n      source.start(0)\n      \n      // Handle completion\n      source.onended = () => {\n        isPlaying.value = false\n      }\n    } catch (error) {\n      console.error('Error playing audio:', error)\n      isPlaying.value = false\n      throw error\n    }\n  }\n  \n  const stopPlayback = () => {\n    if (audioContext.value) {\n      audioContext.value.close()\n      audioContext.value = null\n    }\n    isPlaying.value = false\n  }\n  \n  return {\n    isPlaying,\n    playAudioBuffer,\n    stopPlayback\n  }\n}\n```\n\n2. Create a WebSocket message handler to process incoming audio data:\n```typescript\nimport { ref } from 'vue'\nimport { useAudioPlayback } from './useAudioPlayback'\n\nexport function useWebSocketAudio(wsUrl: string) {\n  const { playAudioBuffer, isPlaying } = useAudioPlayback()\n  const isConnected = ref(false)\n  const transcription = ref('')\n  const websocket = ref<WebSocket | null>(null)\n  \n  const connect = () => {\n    websocket.value = new WebSocket(wsUrl)\n    \n    websocket.value.onopen = () => {\n      isConnected.value = true\n      console.log('WebSocket connected')\n    }\n    \n    websocket.value.onclose = () => {\n      isConnected.value = false\n      console.log('WebSocket disconnected')\n      // Implement reconnection logic\n      setTimeout(connect, 3000)\n    }\n    \n    websocket.value.onerror = (error) => {\n      console.error('WebSocket error:', error)\n    }\n    \n    websocket.value.onmessage = async (event) => {\n      try {\n        // Check if the message is binary (audio) or text (transcription)\n        if (event.data instanceof Blob) {\n          const audioData = await event.data.arrayBuffer()\n          await playAudioBuffer(audioData)\n        } else {\n          // Handle text messages (transcriptions, status updates)\n          const message = JSON.parse(event.data)\n          \n          if (message.type === 'transcription') {\n            transcription.value = message.text\n          }\n          // Handle other message types\n        }\n      } catch (error) {\n        console.error('Error processing WebSocket message:', error)\n      }\n    }\n  }\n  \n  const disconnect = () => {\n    if (websocket.value) {\n      websocket.value.close()\n      websocket.value = null\n    }\n  }\n  \n  return {\n    isConnected,\n    transcription,\n    isPlaying,\n    connect,\n    disconnect\n  }\n}\n```\n\n3. Implement audio queue to handle multiple audio responses\n4. Add volume control and mute functionality\n5. Implement error handling for audio decoding and playback issues\n6. Add visual indicators for playback state",
      "testStrategy": "1. Test audio playback with sample audio buffers\n2. Verify WebSocket message handling for different message types\n3. Test queue management for multiple audio responses\n4. Validate error handling for malformed audio data\n5. Test playback controls (volume, mute)\n6. Verify visual indicators update correctly\n7. Test across different browsers for compatibility",
      "priority": "medium",
      "dependencies": [
        2,
        5
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 7,
      "title": "Implement Main Vue3 Interface",
      "description": "Develop the main Vue3 interface with components for audio control, transcription display, and system status.",
      "details": "1. Create the main App.vue component:\n```vue\n<template>\n  <div class=\"app-container\">\n    <header>\n      <h1>Home Assistant Voice Control</h1>\n      <div class=\"connection-status\" :class=\"{ connected: isConnected }\">\n        {{ isConnected ? 'Connected' : 'Disconnected' }}\n      </div>\n    </header>\n    \n    <main>\n      <div class=\"transcription-container\">\n        <h2>Transcription</h2>\n        <div class=\"transcription-text\">{{ transcription || 'Waiting for speech...' }}</div>\n      </div>\n      \n      <div class=\"visualizer-container\">\n        <AudioVisualizer \n          :media-stream=\"mediaStream\" \n          :width=\"400\" \n          :height=\"100\" \n        />\n      </div>\n      \n      <div class=\"controls-container\">\n        <button \n          class=\"mic-button\" \n          :class=\"{ active: isRecording }\" \n          @click=\"toggleRecording\"\n        >\n          <span class=\"mic-icon\">üé§</span>\n          {{ isRecording ? 'Stop' : 'Start' }}\n        </button>\n      </div>\n      \n      <div class=\"response-container\">\n        <h2>System Response</h2>\n        <div class=\"response-text\">{{ systemResponse }}</div>\n      </div>\n    </main>\n  </div>\n</template>\n\n<script setup lang=\"ts\">\nimport { ref, onMounted, onUnmounted } from 'vue'\nimport { useAudioCapture } from '@/composables/useAudioCapture'\nimport { useWebSocketAudio } from '@/composables/useWebSocketAudio'\nimport AudioVisualizer from '@/components/AudioVisualizer.vue'\n\nconst wsUrl = 'ws://localhost:8000/ws'\nconst { isRecording, startRecording, stopRecording } = useAudioCapture()\nconst { isConnected, transcription, isPlaying, connect, disconnect } = useWebSocketAudio(wsUrl)\n\nconst mediaStream = ref<MediaStream | null>(null)\nconst systemResponse = ref('')\n\nconst toggleRecording = async () => {\n  try {\n    if (isRecording.value) {\n      stopRecording()\n      mediaStream.value = null\n    } else {\n      if (!isConnected.value) {\n        connect()\n      }\n      \n      mediaStream.value = await navigator.mediaDevices.getUserMedia({\n        audio: {\n          channelCount: 1,\n          sampleRate: 16000\n        }\n      })\n      \n      await startRecording(wsUrl)\n    }\n  } catch (error) {\n    console.error('Error toggling recording:', error)\n    // Show error to user\n  }\n}\n\nonMounted(() => {\n  connect()\n})\n\nonUnmounted(() => {\n  stopRecording()\n  disconnect()\n})\n</script>\n\n<style scoped>\n.app-container {\n  max-width: 800px;\n  margin: 0 auto;\n  padding: 20px;\n  font-family: Arial, sans-serif;\n}\n\nheader {\n  display: flex;\n  justify-content: space-between;\n  align-items: center;\n  margin-bottom: 20px;\n}\n\n.connection-status {\n  padding: 5px 10px;\n  border-radius: 4px;\n  background-color: #f44336;\n  color: white;\n}\n\n.connection-status.connected {\n  background-color: #4caf50;\n}\n\n.transcription-container,\n.response-container {\n  margin-bottom: 20px;\n  padding: 15px;\n  border-radius: 8px;\n  background-color: #f5f5f5;\n}\n\n.transcription-text,\n.response-text {\n  min-height: 60px;\n  padding: 10px;\n  border-radius: 4px;\n  background-color: white;\n  border: 1px solid #ddd;\n}\n\n.visualizer-container {\n  margin-bottom: 20px;\n}\n\n.controls-container {\n  display: flex;\n  justify-content: center;\n  margin-bottom: 20px;\n}\n\n.mic-button {\n  display: flex;\n  align-items: center;\n  justify-content: center;\n  padding: 15px 30px;\n  border: none;\n  border-radius: 50px;\n  background-color: #2196f3;\n  color: white;\n  font-size: 18px;\n  cursor: pointer;\n  transition: background-color 0.3s;\n}\n\n.mic-button:hover {\n  background-color: #0b7dda;\n}\n\n.mic-button.active {\n  background-color: #f44336;\n}\n\n.mic-icon {\n  margin-right: 10px;\n  font-size: 24px;\n}\n</style>\n```\n\n2. Create a Pinia store for managing application state:\n```typescript\n// src/store/appStore.ts\nimport { defineStore } from 'pinia'\n\nexport const useAppStore = defineStore('app', {\n  state: () => ({\n    isConnected: false,\n    isRecording: false,\n    isPlaying: false,\n    transcription: '',\n    systemResponse: '',\n    error: null as string | null,\n  }),\n  \n  actions: {\n    setConnectionStatus(status: boolean) {\n      this.isConnected = status\n    },\n    \n    setRecordingStatus(status: boolean) {\n      this.isRecording = status\n    },\n    \n    setPlayingStatus(status: boolean) {\n      this.isPlaying = status\n    },\n    \n    updateTranscription(text: string) {\n      this.transcription = text\n    },\n    \n    updateSystemResponse(text: string) {\n      this.systemResponse = text\n    },\n    \n    setError(error: string | null) {\n      this.error = error\n    },\n    \n    resetState() {\n      this.isRecording = false\n      this.isPlaying = false\n      this.transcription = ''\n      this.systemResponse = ''\n      this.error = null\n    }\n  }\n})\n```\n\n3. Create additional components:\n   - ErrorNotification.vue for displaying errors\n   - ConnectionStatus.vue for showing connection status\n   - AudioControls.vue for audio control buttons\n\n4. Implement responsive design using CSS media queries\n5. Add loading states and transitions\n6. Implement error handling and user feedback",
      "testStrategy": "1. Test component rendering and layout\n2. Verify responsive design on different screen sizes\n3. Test state management with Pinia store\n4. Validate user interactions (button clicks, etc.)\n5. Test error handling and notifications\n6. Verify visual feedback for different states (recording, playing, etc.)\n7. Test accessibility features\n8. Perform cross-browser testing",
      "priority": "medium",
      "dependencies": [
        5,
        6
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 8,
      "title": "Integrate Backend Components",
      "description": "Integrate the Home Assistant client, Gemini Live API client, and WebSocket server in the backend to create a complete processing pipeline.",
      "details": "1. Create a main application class in backend/poc_app/core/app.py to orchestrate the components:\n```python\nclass GeminiHomeAssistantApp:\n    def __init__(self, gemini_api_key: str, ha_url: str, ha_token: str):\n        self.gemini_client = GeminiLiveClient(gemini_api_key)\n        self.ha_client = HomeAssistantClient(ha_url, ha_token)\n        self.active_sessions = {}\n        \n    async def create_session(self, session_id: str):\n        \"\"\"Create a new session with Gemini Live API\"\"\"\n        gemini_session = await self.gemini_client.start_audio_session(HA_FUNCTION_DECLARATIONS)\n        self.active_sessions[session_id] = {\n            \"gemini_session\": gemini_session,\n            \"created_at\": datetime.now(),\n        }\n        return session_id\n        \n    async def process_audio(self, session_id: str, audio_chunk: bytes):\n        \"\"\"Process an audio chunk and return response\"\"\"\n        if session_id not in self.active_sessions:\n            raise ValueError(f\"Session {session_id} not found\")\n            \n        session_data = self.active_sessions[session_id]\n        gemini_session = session_data[\"gemini_session\"]\n        \n        # Process audio with Gemini\n        response = await self.gemini_client.process_audio_chunk(gemini_session, audio_chunk)\n        \n        # Check for function calls\n        function_result = None\n        if response.candidates and response.candidates[0].content.parts:\n            function_result = await self.gemini_client.handle_function_call(response, self.ha_client)\n        \n        # Get transcription\n        transcription = response.text if hasattr(response, 'text') else \"\"\n        \n        # Get audio response if function was called\n        audio_response = None\n        if function_result:\n            audio_response = await self.gemini_client.get_audio_response(gemini_session, function_result)\n        \n        return {\n            \"transcription\": transcription,\n            \"audio_response\": audio_response,\n            \"function_result\": function_result\n        }\n        \n    def close_session(self, session_id: str):\n        \"\"\"Close a session\"\"\"\n        if session_id in self.active_sessions:\n            del self.active_sessions[session_id]\n            \n    async def cleanup_old_sessions(self, max_age_minutes: int = 10):\n        \"\"\"Clean up sessions older than max_age_minutes\"\"\"\n        now = datetime.now()\n        sessions_to_remove = []\n        \n        for session_id, session_data in self.active_sessions.items():\n            age = (now - session_data[\"created_at\"]).total_seconds() / 60\n            if age > max_age_minutes:\n                sessions_to_remove.append(session_id)\n                \n        for session_id in sessions_to_remove:\n            self.close_session(session_id)\n```\n\n2. Update the WebSocket endpoint in main.py to use the application class:\n```python\nfrom fastapi import FastAPI, WebSocket, WebSocketDisconnect\nfrom uuid import uuid4\nfrom core.app import GeminiHomeAssistantApp\nimport os\nfrom dotenv import load_dotenv\n\nload_dotenv()\n\napp = FastAPI()\n\n# Initialize the application\ngemini_app = GeminiHomeAssistantApp(\n    gemini_api_key=os.getenv(\"GEMINI_API_KEY\"),\n    ha_url=os.getenv(\"HA_URL\"),\n    ha_token=os.getenv(\"HA_LLAT\")\n)\n\n@app.websocket(\"/ws\")\nasync def websocket_endpoint(websocket: WebSocket):\n    await websocket.accept()\n    \n    # Create a unique session ID\n    session_id = str(uuid4())\n    await gemini_app.create_session(session_id)\n    \n    try:\n        while True:\n            # Receive audio data\n            audio_data = await websocket.receive_bytes()\n            \n            # Process audio\n            result = await gemini_app.process_audio(session_id, audio_data)\n            \n            # Send transcription\n            if result[\"transcription\"]:\n                await websocket.send_json({\n                    \"type\": \"transcription\",\n                    \"text\": result[\"transcription\"]\n                })\n            \n            # Send audio response if available\n            if result[\"audio_response\"]:\n                await websocket.send_bytes(result[\"audio_response\"])\n                \n            # Send function result if available\n            if result[\"function_result\"]:\n                await websocket.send_json({\n                    \"type\": \"function_result\",\n                    \"result\": result[\"function_result\"]\n                })\n                \n    except WebSocketDisconnect:\n        gemini_app.close_session(session_id)\n        \n    except Exception as e:\n        logger.error(f\"Error in WebSocket: {str(e)}\")\n        await websocket.close(code=1011)\n        gemini_app.close_session(session_id)\n\n# Background task to clean up old sessions\n@app.on_event(\"startup\")\nasync def startup_event():\n    asyncio.create_task(cleanup_sessions_task())\n\nasync def cleanup_sessions_task():\n    while True:\n        await gemini_app.cleanup_old_sessions()\n        await asyncio.sleep(60)  # Check every minute\n```\n\n3. Implement error handling and logging throughout the integration\n4. Add session management and cleanup\n5. Implement message protocol for different types of responses\n6. Add configuration validation using Pydantic models",
      "testStrategy": "1. Test the complete processing pipeline with sample audio data\n2. Verify session creation, management, and cleanup\n3. Test error handling and recovery\n4. Validate message protocol between components\n5. Test integration with both Gemini Live API and Home Assistant\n6. Measure and optimize performance (latency, memory usage)\n7. Test with various audio inputs and commands",
      "priority": "high",
      "dependencies": [
        2,
        3,
        4
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 9,
      "title": "Implement Error Handling and Logging",
      "description": "Implement comprehensive error handling and logging throughout the application to ensure reliability and facilitate debugging.",
      "details": "1. Create a logging module in backend/poc_app/core/logging.py:\n```python\nimport logging\nimport json\nfrom datetime import datetime\nfrom pathlib import Path\n\nclass CustomFormatter(logging.Formatter):\n    def format(self, record):\n        log_record = {\n            \"timestamp\": datetime.utcnow().isoformat(),\n            \"level\": record.levelname,\n            \"message\": record.getMessage(),\n            \"module\": record.module,\n            \"function\": record.funcName,\n            \"line\": record.lineno\n        }\n        \n        if hasattr(record, 'extra'):\n            log_record.update(record.extra)\n            \n        if record.exc_info:\n            log_record[\"exception\"] = self.formatException(record.exc_info)\n            \n        return json.dumps(log_record)\n\ndef setup_logging(log_level=logging.INFO, log_file=None):\n    logger = logging.getLogger(\"poc_app\")\n    logger.setLevel(log_level)\n    \n    # Clear existing handlers\n    logger.handlers = []\n    \n    # Console handler\n    console_handler = logging.StreamHandler()\n    console_handler.setFormatter(CustomFormatter())\n    logger.addHandler(console_handler)\n    \n    # File handler if specified\n    if log_file:\n        log_path = Path(log_file)\n        log_path.parent.mkdir(parents=True, exist_ok=True)\n        \n        file_handler = logging.FileHandler(log_file)\n        file_handler.setFormatter(CustomFormatter())\n        logger.addHandler(file_handler)\n    \n    return logger\n```\n\n2. Create error handling middleware for FastAPI in backend/poc_app/core/middleware.py:\n```python\nfrom fastapi import Request, status\nfrom fastapi.responses import JSONResponse\nfrom fastapi.exceptions import RequestValidationError\nfrom starlette.exceptions import HTTPException as StarletteHTTPException\nimport logging\nimport traceback\n\nlogger = logging.getLogger(\"poc_app\")\n\nasync def exception_handler(request: Request, exc: Exception):\n    error_id = str(uuid.uuid4())\n    \n    error_detail = {\n        \"error_id\": error_id,\n        \"type\": exc.__class__.__name__,\n        \"detail\": str(exc)\n    }\n    \n    logger.error(\n        f\"Unhandled exception: {exc}\",\n        extra={\n            \"error_id\": error_id,\n            \"path\": request.url.path,\n            \"method\": request.method,\n            \"traceback\": traceback.format_exc()\n        }\n    )\n    \n    return JSONResponse(\n        status_code=status.HTTP_500_INTERNAL_SERVER_ERROR,\n        content={\"error\": error_detail}\n    )\n\nasync def http_exception_handler(request: Request, exc: StarletteHTTPException):\n    logger.warning(\n        f\"HTTP exception: {exc.detail}\",\n        extra={\n            \"status_code\": exc.status_code,\n            \"path\": request.url.path,\n            \"method\": request.method\n        }\n    )\n    \n    return JSONResponse(\n        status_code=exc.status_code,\n        content={\"error\": {\"detail\": exc.detail}}\n    )\n\nasync def validation_exception_handler(request: Request, exc: RequestValidationError):\n    logger.warning(\n        f\"Validation error: {exc}\",\n        extra={\n            \"path\": request.url.path,\n            \"method\": request.method,\n            \"errors\": exc.errors()\n        }\n    )\n    \n    return JSONResponse(\n        status_code=status.HTTP_422_UNPROCESSABLE_ENTITY,\n        content={\"error\": {\"detail\": exc.errors()}}\n    )\n```\n\n3. Create error handling in frontend/src/utils/errorHandler.ts:\n```typescript\nimport { useAppStore } from '@/store/appStore'\n\nexport enum ErrorSeverity {\n  INFO = 'info',\n  WARNING = 'warning',\n  ERROR = 'error',\n  CRITICAL = 'critical'\n}\n\nexport interface ErrorDetails {\n  message: string\n  severity: ErrorSeverity\n  code?: string\n  source?: string\n  timestamp?: Date\n}\n\nexport class AppError extends Error {\n  details: ErrorDetails\n  \n  constructor(message: string, severity: ErrorSeverity = ErrorSeverity.ERROR, code?: string, source?: string) {\n    super(message)\n    this.name = 'AppError'\n    this.details = {\n      message,\n      severity,\n      code,\n      source,\n      timestamp: new Date()\n    }\n  }\n}\n\nexport function handleError(error: Error | AppError | unknown): ErrorDetails {\n  const store = useAppStore()\n  let errorDetails: ErrorDetails\n  \n  if (error instanceof AppError) {\n    errorDetails = error.details\n  } else if (error instanceof Error) {\n    errorDetails = {\n      message: error.message,\n      severity: ErrorSeverity.ERROR,\n      timestamp: new Date()\n    }\n  } else {\n    errorDetails = {\n      message: 'An unknown error occurred',\n      severity: ErrorSeverity.ERROR,\n      timestamp: new Date()\n    }\n  }\n  \n  // Log to console\n  console.error('Application error:', errorDetails)\n  \n  // Update store\n  store.setError(errorDetails.message)\n  \n  return errorDetails\n}\n\nexport function clearError(): void {\n  const store = useAppStore()\n  store.setError(null)\n}\n```\n\n4. Create an error notification component in frontend/src/components/ErrorNotification.vue:\n```vue\n<template>\n  <div v-if=\"error\" class=\"error-notification\" :class=\"severityClass\">\n    <div class=\"error-content\">\n      <span class=\"error-icon\">‚ö†Ô∏è</span>\n      <span class=\"error-message\">{{ error }}</span>\n    </div>\n    <button class=\"close-button\" @click=\"clearError\">√ó</button>\n  </div>\n</template>\n\n<script setup lang=\"ts\">\nimport { computed } from 'vue'\nimport { useAppStore } from '@/store/appStore'\nimport { clearError } from '@/utils/errorHandler'\n\nconst store = useAppStore()\n\nconst error = computed(() => store.error)\n\nconst severityClass = computed(() => {\n  // In a real app, you would determine this based on error severity\n  return 'severity-error'\n})\n</script>\n\n<style scoped>\n.error-notification {\n  position: fixed;\n  top: 20px;\n  right: 20px;\n  padding: 15px;\n  border-radius: 4px;\n  display: flex;\n  align-items: center;\n  justify-content: space-between;\n  max-width: 400px;\n  box-shadow: 0 4px 6px rgba(0, 0, 0, 0.1);\n  z-index: 1000;\n  animation: slide-in 0.3s ease-out;\n}\n\n.severity-error {\n  background-color: #ffebee;\n  border-left: 4px solid #f44336;\n  color: #d32f2f;\n}\n\n.severity-warning {\n  background-color: #fff8e1;\n  border-left: 4px solid #ffc107;\n  color: #ff8f00;\n}\n\n.severity-info {\n  background-color: #e3f2fd;\n  border-left: 4px solid #2196f3;\n  color: #1976d2;\n}\n\n.error-content {\n  display: flex;\n  align-items: center;\n}\n\n.error-icon {\n  margin-right: 10px;\n  font-size: 20px;\n}\n\n.close-button {\n  background: none;\n  border: none;\n  font-size: 20px;\n  cursor: pointer;\n  margin-left: 10px;\n}\n\n@keyframes slide-in {\n  from {\n    transform: translateX(100%);\n    opacity: 0;\n  }\n  to {\n    transform: translateX(0);\n    opacity: 1;\n  }\n}\n</style>\n```\n\n5. Update main.py to use the error handling middleware:\n```python\nfrom fastapi import FastAPI\nfrom fastapi.exceptions import RequestValidationError\nfrom starlette.exceptions import HTTPException as StarletteHTTPException\nfrom core.middleware import exception_handler, http_exception_handler, validation_exception_handler\nfrom core.logging import setup_logging\n\n# Setup logging\nlogger = setup_logging(log_file=\"logs/app.log\")\n\napp = FastAPI()\n\n# Add exception handlers\napp.add_exception_handler(Exception, exception_handler)\napp.add_exception_handler(StarletteHTTPException, http_exception_handler)\napp.add_exception_handler(RequestValidationError, validation_exception_handler)\n```\n\n6. Implement retry logic for external API calls\n7. Add graceful degradation for non-critical features\n8. Implement circuit breaker pattern for external dependencies",
      "testStrategy": "1. Test error handling for various error scenarios\n2. Verify logging captures all relevant information\n3. Test error notification component in the frontend\n4. Validate retry logic for external API calls\n5. Test graceful degradation of features\n6. Verify circuit breaker functionality\n7. Test error recovery and system stability\n8. Validate error messages are user-friendly",
      "priority": "medium",
      "dependencies": [
        2,
        3,
        4,
        5,
        6,
        7,
        8
      ],
      "status": "pending",
      "subtasks": []
    },
    {
      "id": 10,
      "title": "Implement Testing and Documentation",
      "description": "Create comprehensive tests and documentation for the application to ensure reliability and facilitate future development.",
      "details": "1. Create backend unit tests in backend/tests/:\n```python\n# tests/test_ha_client.py\nimport pytest\nfrom unittest.mock import AsyncMock, patch\nfrom poc_app.ha_client.client import HomeAssistantClient\n\n@pytest.fixture\ndef mock_ha_client():\n    client = HomeAssistantClient(\"http://test-ha.local:8123\", \"test-token\")\n    client.client = AsyncMock()\n    return client\n\n@pytest.mark.asyncio\nasync def test_get_entity_state(mock_ha_client):\n    # Mock response\n    mock_response = AsyncMock()\n    mock_response.json.return_value = {\"entity_id\": \"light.test\", \"state\": \"on\"}\n    mock_ha_client.client.get.return_value = mock_response\n    \n    # Call method\n    result = await mock_ha_client.get_entity_state(\"light.test\")\n    \n    # Assertions\n    mock_ha_client.client.get.assert_called_once_with(\n        \"http://test-ha.local:8123/api/states/light.test\"\n    )\n    assert result[\"entity_id\"] == \"light.test\"\n    assert result[\"state\"] == \"on\"\n\n@pytest.mark.asyncio\nasync def test_control_light(mock_ha_client):\n    # Mock response\n    mock_response = AsyncMock()\n    mock_response.json.return_value = {\"success\": True}\n    mock_ha_client.client.post.return_value = mock_response\n    \n    # Call method\n    result = await mock_ha_client.control_light(\"light.test\", \"on\", brightness=128)\n    \n    # Assertions\n    mock_ha_client.client.post.assert_called_once_with(\n        \"http://test-ha.local:8123/api/services/light/turn_on\",\n        json={\"entity_id\": \"light.test\", \"brightness\": 128}\n    )\n    assert result[\"success\"] == True\n```\n\n2. Create frontend unit tests in frontend/tests/:\n```typescript\n// tests/unit/composables/useAudioCapture.spec.ts\nimport { describe, it, expect, vi, beforeEach, afterEach } from 'vitest'\nimport { useAudioCapture } from '@/composables/useAudioCapture'\nimport { ref } from 'vue'\n\n// Mock navigator.mediaDevices\nconst mockMediaStream = {\n  getTracks: vi.fn().mockReturnValue([{ stop: vi.fn() }])\n}\n\nconst mockAudioContext = {\n  createScriptProcessor: vi.fn().mockReturnValue({\n    connect: vi.fn(),\n    disconnect: vi.fn()\n  }),\n  createMediaStreamSource: vi.fn().mockReturnValue({\n    connect: vi.fn()\n  }),\n  destination: {},\n  close: vi.fn()\n}\n\nvi.mock('vue', async () => {\n  const actual = await vi.importActual('vue')\n  return {\n    ...actual,\n    onUnmounted: vi.fn()\n  }\n})\n\ndescribe('useAudioCapture', () => {\n  beforeEach(() => {\n    // Mock global objects\n    global.navigator.mediaDevices = {\n      getUserMedia: vi.fn().mockResolvedValue(mockMediaStream)\n    }\n    global.AudioContext = vi.fn().mockImplementation(() => mockAudioContext)\n    global.WebSocket = vi.fn().mockImplementation(() => ({\n      onopen: null,\n      onerror: null,\n      send: vi.fn(),\n      close: vi.fn()\n    }))\n  })\n  \n  afterEach(() => {\n    vi.clearAllMocks()\n  })\n  \n  it('should initialize with isRecording set to false', () => {\n    const { isRecording } = useAudioCapture()\n    expect(isRecording.value).toBe(false)\n  })\n  \n  it('should start recording when startRecording is called', async () => {\n    const { isRecording, startRecording } = useAudioCapture()\n    \n    await startRecording('ws://localhost:8000/ws')\n    \n    expect(isRecording.value).toBe(true)\n    expect(global.navigator.mediaDevices.getUserMedia).toHaveBeenCalledWith({\n      audio: {\n        channelCount: 1,\n        sampleRate: 16000\n      }\n    })\n    expect(global.AudioContext).toHaveBeenCalled()\n    expect(global.WebSocket).toHaveBeenCalledWith('ws://localhost:8000/ws')\n  })\n  \n  it('should stop recording when stopRecording is called', async () => {\n    const { isRecording, startRecording, stopRecording } = useAudioCapture()\n    \n    await startRecording('ws://localhost:8000/ws')\n    stopRecording()\n    \n    expect(isRecording.value).toBe(false)\n    expect(mockMediaStream.getTracks).toHaveBeenCalled()\n    expect(mockAudioContext.close).toHaveBeenCalled()\n  })\n})\n```\n\n3. Create integration tests for the backend:\n```python\n# tests/test_integration.py\nimport pytest\nfrom fastapi.testclient import TestClient\nfrom unittest.mock import patch, AsyncMock\nfrom poc_app.main import app\n\n@pytest.fixture\ndef test_client():\n    return TestClient(app)\n\ndef test_health_endpoint(test_client):\n    response = test_client.get(\"/health\")\n    assert response.status_code == 200\n    assert response.json() == {\"status\": \"ok\"}\n\n@pytest.mark.asyncio\nasync def test_websocket_connection():\n    with patch('poc_app.core.app.GeminiHomeAssistantApp.create_session', new_callable=AsyncMock) as mock_create_session:\n        mock_create_session.return_value = \"test-session\"\n        \n        with TestClient(app).websocket_connect(\"/ws\") as websocket:\n            # Test connection is established\n            assert websocket.accepted\n            mock_create_session.assert_called_once()\n```\n\n4. Create end-to-end tests for the frontend:\n```typescript\n// tests/e2e/app.spec.ts\nimport { test, expect } from '@playwright/test'\n\ntest('basic app functionality', async ({ page }) => {\n  // Navigate to the app\n  await page.goto('/')\n  \n  // Check initial state\n  await expect(page.locator('h1')).toHaveText('Home Assistant Voice Control')\n  await expect(page.locator('.connection-status')).toHaveText('Disconnected')\n  await expect(page.locator('.transcription-text')).toHaveText('Waiting for speech...')\n  \n  // Test microphone button\n  const micButton = page.locator('.mic-button')\n  await expect(micButton).toHaveText('Start')\n  \n  // Click the mic button (will request permissions in a real browser)\n  await micButton.click()\n  \n  // In a real test, we would need to handle permission dialogs\n  // and mock WebSocket connections\n})\n```\n\n5. Create API documentation using FastAPI's built-in Swagger UI\n\n6. Create a comprehensive README.md:\n```markdown\n# Home Assistant Control with Gemini Live API and Vue3 Interface\n\nA proof of concept application that allows controlling Home Assistant devices using voice commands processed by Google's Gemini Live API, with a modern Vue3 interface.\n\n## Features\n\n- Voice control for Home Assistant devices\n- Real-time bidirectional audio streaming\n- Natural language processing with Gemini Live API\n- Modern and responsive Vue3 interface\n- Privacy-focused approach\n\n## Prerequisites\n\n- Python 3.11+\n- Node.js 16+\n- Google Cloud account with Gemini API access\n- Home Assistant instance with API access\n\n## Setup\n\n### Backend\n\n1. Clone the repository\n2. Navigate to the backend directory: `cd poc_gemini_ha/backend`\n3. Create a virtual environment: `python -m venv venv`\n4. Activate the virtual environment:\n   - Windows: `venv\\Scripts\\activate`\n   - Unix/MacOS: `source venv/bin/activate`\n5. Install dependencies: `pip install -r requirements.txt`\n6. Create a `.env` file with the following variables:\n   ```\n   GEMINI_API_KEY=\"your_gemini_api_key\"\n   HA_URL=\"http://homeassistant.local:8123\"\n   HA_LLAT=\"your_home_assistant_token\"\n   AUDIO_SAMPLE_RATE_GEMINI=16000\n   AUDIO_CHANNELS_GEMINI=1\n   ```\n7. Start the server: `uvicorn poc_app.main:app --reload`\n\n### Frontend\n\n1. Navigate to the frontend directory: `cd poc_gemini_ha/frontend`\n2. Install dependencies: `npm install`\n3. Create a `.env` file with the following variables:\n   ```\n   VITE_API_URL=\"http://localhost:8000\"\n   VITE_WS_URL=\"ws://localhost:8000/ws\"\n   ```\n4. Start the development server: `npm run dev`\n\n## Usage\n\n1. Open the application in your browser\n2. Click the microphone button to start recording\n3. Speak a command (e.g., \"Turn on the living room lights\")\n4. The system will process your command and control Home Assistant accordingly\n5. You will receive an audio response confirming the action\n\n## Development\n\n### Running Tests\n\n- Backend: `pytest`\n- Frontend: `npm run test:unit`\n\n### Building for Production\n\n- Backend: Package as needed (Docker recommended)\n- Frontend: `npm run build`\n\n## Architecture\n\nThe application consists of the following components:\n\n1. **Frontend (Vue3)**: Handles user interface, audio capture, and playback\n2. **Backend (FastAPI)**: Orchestrates communication between components\n3. **Gemini Client**: Interfaces with Google's Gemini Live API\n4. **Home Assistant Client**: Communicates with Home Assistant API\n\n## License\n\nMIT\n```\n\n7. Create developer documentation for each module\n8. Add inline code documentation and type hints",
      "testStrategy": "1. Run unit tests for backend components\n2. Run unit tests for frontend components\n3. Run integration tests for backend API\n4. Run end-to-end tests for the complete application\n5. Verify documentation is accurate and comprehensive\n6. Test README instructions by following them on a clean environment\n7. Validate API documentation with sample requests\n8. Ensure all code has proper type hints and documentation",
      "priority": "low",
      "dependencies": [
        1,
        2,
        3,
        4,
        5,
        6,
        7,
        8,
        9
      ],
      "status": "pending",
      "subtasks": []
    }
  ]
}